# Dual-Agent DevOps: Gemini vs OpenAI
%BADGE%

A reproducible benchmark comparing **Google Gemini (SDK)** and **OpenAI (CLI/SDK)** on developer tasks:

- **Issue triage** → labels, priority, rationale  
- **PR review** → rubric sections, issues raised, tests suggested

This repo favors **scriptable, deterministic runs** (JSON/CSV/MD outputs) and clean CI artifacts for forensic comparisons.

---

## Why this matters (SWE interview + research)
- **Systems engineering**: Agent vs API orchestration, reproducibility, CI ergonomics
- **Evaluation**: task-specific metrics (label overlap, rubric coverage, latency)
- **Practicality**: GitHub-ready runners, secrets hygiene, artifacted results

---

## Setup (local)
**Prereqs**
- Python 3.12+, `python3-venv`
- Node not required for benchmarks (Gemini CLI proved less scriptable; we use SDK)
- Accounts/keys:
  - `OPENAI_API_KEY`
  - `GEMINI_API_KEY` (Google AI Studio)

**Create venv + install**
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip openai google-genai pydantic
export OPENAI_API_KEY="sk-..."
# Option A: export in-shell
export GEMINI_API_KEY="..."
# Option B: or store once at ~/.gemini/.env and source automatically
OPENAI_API_KEY="$OPENAI_API_KEY" bin/run_triage_all.sh
OPENAI_API_KEY="$OPENAI_API_KEY" bin/run_review_all.sh
results/summaries/triage_metrics.csv
results/summaries/triage_compare.md
results/summaries/review_metrics.csv
results/summaries/review_compare.md
results/raw/**  # per-task JSON/MD
CI (GitHub Actions)

Add repo secrets:

OPENAI_API_KEY

GEMINI_API_KEY

Workflow: .github/workflows/triage-compare.yml
Triggers: PRs, pushes to main, manual dispatch.

Artifacts:

triage-summaries (CSV + MD)

review-summaries (CSV + MD)

raw-outputs (all raw files)
datasets/
  issues/        # source issues (.md). CI commits only files prefixed with ci_
  prs/           # source diffs (.diff). CI commits only files prefixed with ci_
prompts/         # task prompts
runners/         # stack-specific runners (OpenAI & Gemini SDK)
scripts/         # evaluation + comparison
results/         # raw outputs + summaries (ignored in git, CI artifacts hold them)

!datasets/issues/ci_*.md
!datasets/prs/ci_*.diff
Metrics
Triage

labels_jaccard: |A∩B| / |A∪B| (OpenAI vs Gemini labels)

priority_delta: |prio_openai − prio_gemini|

latency_s: wall-clock seconds (per runner)

rationale_len: characters in rationale (proxy for verbosity)

Review

rubric_sections_present: count of # Summary, # Major Issues, # Minor Issues, # Tests Suggested, # Security, # Performance

major_issues_count: bullet/numbered items under “Major Issues”

tests_suggested_count: bullet/numbered items under “Tests Suggested”

latency_s, lines, chars
Results (latest run)
Triage (OpenAI vs Gemini)

%TRIAGE_TABLE%

Review (OpenAI vs Gemini)

%REVIEW_TABLE%
Repro tips

Pin versions in CI for strict reproducibility.

Keep prompts in prompts/ and version them; sweep temperatures if doing research.

Add your own issues/diffs under datasets/ (use ci_ prefixes for CI).

License

MIT
